[
  {
    "topic_name": "Lecture 3: Loss functions",
    "content": ""
  },
  {
    "topic_name": "Madhavan Mukund",
    "content": "https://www.cmi.ac.in/ ~ madhavan\n\nAdvanced Machine Learning September-December 2021\n\n- Supervised learning estimates parameters for a model based on training data\n- Parameter estimate is through gradient descent\n- Define a loss function measuring the error with respect to training data\n- Compute gradients with respect to each parameter\n- Adjust parameters by a small step in direction opposite to gradients\n- Typical loss functions include mean squared error (MSE) and cross entropy\n- How do arrive at these loss functions?"
  },
  {
    "topic_name": "Maximum likelihood estimators (MLE)",
    "content": "- Build a model M from training data D = { ( x 1 , y 1 , ) , ( x 2 , y 2 , ) , . . . , ( x n , y n ) }\n- Learning - define M by computing parameters θ\n- Model predicts value ˆ on input y x i with probability P model (ˆ y | x i , θ )\n- Probability of predicting correct value is P model ( y i | x i , θ )\n- Likelihood is n ∏ i =1 P model ( y i | x i , θ )\n- Find M that maximizes the likelihood"
  },
  {
    "topic_name": "Log likelihood",
    "content": "- Maximize the likelihood\n\nP\n\nmodel\n\n(\n\ny\n\ni\n\n|\n\nx\n\ni\n\n, θ\n\n)\n\ni =1\n\n- log is an increasing function, so we can equivalently maximize log likelihood\n\n<!-- formula-not-decoded -->\n\n- Rewrite log likelihood as a sum\n\n<!-- formula-not-decoded -->\n\nn\n\n∏"
  },
  {
    "topic_name": "Maximizing Log likelihood",
    "content": "<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n- Log likelihood is a function of the learned parameters θ\n\n<!-- formula-not-decoded -->\n\n- To maximize, find an optimum value of θ : ∂ L ( θ ) ∂θ = 0\n\n- Let X = { x 1 , x 2 , . . . , x k } with a probability distribution P\n- Entropy is defined as H P ( ) = -k ∑ i =1 P x ( i ) log P x ( i )\n- Average number of bits to encode each element of X\n- Given two distributions P and Q over X , cross entropy is defined as H P Q ( , ) = -k ∑ i =1 P x ( i ) log Q x ( i )\n- Imagine an encoding based on Q where true distribution is P\n- Again, average number of bits to encode each element of X\n\nglyph[negationslash]\n\n- Note that cross entropy is not symmetric: H P Q ( , ) = H Q P ( , )"
  },
  {
    "topic_name": "Cross entropy and MLE",
    "content": "- Maximum likelihood estimator (MLE) - maximize\n\n<!-- formula-not-decoded -->\n\n- P model is an estimate for the true distribution P data\n\n<!-- formula-not-decoded -->\n\n- H P ( data , P model ) = -L ( θ )\n- Minimizing cross entropy is the same as maximizing likelihood\n- The 'cross entropy loss function' is a special form of this generic observation"
  },
  {
    "topic_name": "Regression and MSE loss",
    "content": "- Training input is { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x n , y n ) }\n- Noisy outputs from a linear function\n- y i = w T x i + glyph[epsilon1]\n- glyph[epsilon1] ∼ N (0 , σ 2 ) : Gaussian noise, mean 0, fixed variance σ 2\n- y i ∼ N ( µ , σ i 2 ), µ i = w T x i\n- Model gives us an estimate for w , so regression learns µ i for each x i\n- P model ( y i | x i , θ ) = 1 √ 2 πσ 2 e -( y -µ i ) 2 2 σ 2 = 1 √ 2 πσ 2 e -( y -w T x i ) 2 2 σ 2\n- Log likelihood (assuming natural logarithm)\n\n<!-- formula-not-decoded -->"
  },
  {
    "topic_name": "Regression and MSE loss",
    "content": "<!-- formula-not-decoded -->\n\n- w T x i is predicted value ˆ y i\n- To maximize L ( θ ) with respect to w , ignore all terms that do not depend on w\n- Optimum value of w is given by\n\n<!-- formula-not-decoded -->\n\n- Assuming data points are generated by linear function and then perturbed by Gaussian noise, MSE is the 'correct' loss function to maximize likelihood (and minimize cross entropy)"
  },
  {
    "topic_name": "Binary classification",
    "content": "- Compute linear output z i = w T x i , then apply sigmoid σ ( z ) = 1 -z\n- 1 + e Let a i = σ ( z i ). So, P model ( y i = 1) = a i , P model ( y i = 0) = 1 -a i Cross entropy: n ∑ ∑ i =1 j ∈{ 0 1 , } P data ( y i = )log( j P model ( y i = j | x i , θ )) Expand: n ∑ i =1 P data ( y i = 0) log P model ( y i = 0 | x i , θ ) + P data ( y i = 1) log P model ( y i = 1 | x i , θ ) Equivalently, n ∑ i =1 (1 -y i ) · log(1 -a i ) + y i · log a i\n- Recommended loss function, directly minimizes cross entropy\n\n- Our goal is to find a maximum likelihood estimator\n- Gradient descent uses a loss function to optimize parameters\n- Finding MLE is equivalent to minimizing cross entropy H P ( data , P model )\n- Applying this to a given situation, we arrive at concrete loss functions\n- Mean square error for regression\n- 'Cross entropy' for binary classification"
  }
]