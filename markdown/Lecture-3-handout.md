## Lecture 3: Loss functions

## Madhavan Mukund

https://www.cmi.ac.in/ ~ madhavan

Advanced Machine Learning September-December 2021

- Supervised learning estimates parameters for a model based on training data
- Parameter estimate is through gradient descent
- Define a loss function measuring the error with respect to training data
- Compute gradients with respect to each parameter
- Adjust parameters by a small step in direction opposite to gradients
- Typical loss functions include mean squared error (MSE) and cross entropy
- How do arrive at these loss functions?

## Maximum likelihood estimators (MLE)

- Build a model M from training data D = { ( x 1 , y 1 , ) , ( x 2 , y 2 , ) , . . . , ( x n , y n ) }
- Learning - define M by computing parameters θ
- Model predicts value ˆ on input y x i with probability P model (ˆ y | x i , θ )
- Probability of predicting correct value is P model ( y i | x i , θ )
- Likelihood is n ∏ i =1 P model ( y i | x i , θ )
- Find M that maximizes the likelihood

## Log likelihood

- Maximize the likelihood

P

model

(

y

i

|

x

i

, θ

)

i =1

- log is an increasing function, so we can equivalently maximize log likelihood

<!-- formula-not-decoded -->

- Rewrite log likelihood as a sum

<!-- formula-not-decoded -->

n

∏

## Maximizing Log likelihood

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- Log likelihood is a function of the learned parameters θ

<!-- formula-not-decoded -->

- To maximize, find an optimum value of θ : ∂ L ( θ ) ∂θ = 0

- Let X = { x 1 , x 2 , . . . , x k } with a probability distribution P
- Entropy is defined as H P ( ) = -k ∑ i =1 P x ( i ) log P x ( i )
- Average number of bits to encode each element of X
- Given two distributions P and Q over X , cross entropy is defined as H P Q ( , ) = -k ∑ i =1 P x ( i ) log Q x ( i )
- Imagine an encoding based on Q where true distribution is P
- Again, average number of bits to encode each element of X

glyph[negationslash]

- Note that cross entropy is not symmetric: H P Q ( , ) = H Q P ( , )

## Cross entropy and MLE

- Maximum likelihood estimator (MLE) - maximize

<!-- formula-not-decoded -->

- P model is an estimate for the true distribution P data

<!-- formula-not-decoded -->

- H P ( data , P model ) = -L ( θ )
- Minimizing cross entropy is the same as maximizing likelihood
- The 'cross entropy loss function' is a special form of this generic observation

## Regression and MSE loss

- Training input is { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x n , y n ) }
- Noisy outputs from a linear function
- y i = w T x i + glyph[epsilon1]
- glyph[epsilon1] ∼ N (0 , σ 2 ) : Gaussian noise, mean 0, fixed variance σ 2
- y i ∼ N ( µ , σ i 2 ), µ i = w T x i
- Model gives us an estimate for w , so regression learns µ i for each x i
- P model ( y i | x i , θ ) = 1 √ 2 πσ 2 e -( y -µ i ) 2 2 σ 2 = 1 √ 2 πσ 2 e -( y -w T x i ) 2 2 σ 2
- Log likelihood (assuming natural logarithm)

<!-- formula-not-decoded -->

## Regression and MSE loss

<!-- formula-not-decoded -->

- w T x i is predicted value ˆ y i
- To maximize L ( θ ) with respect to w , ignore all terms that do not depend on w
- Optimum value of w is given by

<!-- formula-not-decoded -->

- Assuming data points are generated by linear function and then perturbed by Gaussian noise, MSE is the 'correct' loss function to maximize likelihood (and minimize cross entropy)

## Binary classification

- Compute linear output z i = w T x i , then apply sigmoid σ ( z ) = 1 -z
- 1 + e Let a i = σ ( z i ). So, P model ( y i = 1) = a i , P model ( y i = 0) = 1 -a i Cross entropy: n ∑ ∑ i =1 j ∈{ 0 1 , } P data ( y i = )log( j P model ( y i = j | x i , θ )) Expand: n ∑ i =1 P data ( y i = 0) log P model ( y i = 0 | x i , θ ) + P data ( y i = 1) log P model ( y i = 1 | x i , θ ) Equivalently, n ∑ i =1 (1 -y i ) · log(1 -a i ) + y i · log a i
- Recommended loss function, directly minimizes cross entropy

- Our goal is to find a maximum likelihood estimator
- Gradient descent uses a loss function to optimize parameters
- Finding MLE is equivalent to minimizing cross entropy H P ( data , P model )
- Applying this to a given situation, we arrive at concrete loss functions
- Mean square error for regression
- 'Cross entropy' for binary classification